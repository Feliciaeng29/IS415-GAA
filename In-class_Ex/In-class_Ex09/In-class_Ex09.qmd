---
title: "In class Exercise 9"
---

## Installing packages

rpart and rpart.plot: recursive conditioning
ggstatsplot: correlation analysis

```{r}
pacman::p_load(sf, spdep, GWmodel, 
               SpatialML, tmap, 
               tidymodels, tidyverse, 
               gtsummary, 
               rpart, rpart.plot, 
               ggstatsplot, performance)
```

Reading the input data sets. It is in simple feature data frame.

```{r}
rs_sf <- read_rds("data/rds/HDB_resale.rds")
```

Next, we reveal the properties of *rs_sf* object.

```{r}
rs_sf
```

## Data Sampling

The entire data are split into training and test data sets with 65% and 35% respectively by using *initial_split()* of **rsample** package. rsample is one of the package of tigymodels.

Always remember to put set.seed(1234), if not the result will not be consistent.
Prop explain how you split the data how many perc test and training

```{r}
set.seed(1234)
resale_split <- initial_split(
  rs_sf, 
  prop = 5/10,)
train_sf <- training(resale_split)
test_sf <- testing(resale_split)
```

### Converting from sf objects to data.frame gwModel ans spatial ML libraries require to input data

```{r}
train_df <- train_sf %>%
  st_drop_geometry() %>%
  as.data.frame()

test_df <- test_sf %>%
  st_drop_geometry() %>%
  as.data.frame()
```

Save the output files

Optional:
write_rds(train_df, "data/model/train_df.rds")
write_rds(test_df, "data/model/test_df.rds")


## Retrieving stored data

Optional:
train_df <- read_rds("data/model/train_df.rds")
test_df <- read_rds("data/model/test_df.rds")

## Computing correlation Matrix

```{r}
#| fig-width: 12

rs_sf1 <- rs_sf %>%
  st_drop_geometry()
ggcorrmat(rs_sf1[, 2:17])
```

## **Building a non-spatial multiple linear regression**

```{r}
rs_mlr <- lm(RESALE_PRICE ~ FLOOR_AREA_SQM +
                  STOREY_ORDER + REMAINING_LEASE_MTHS +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + PROX_CHAS +
                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_df)
summary(rs_mlr)
```

## Revisiting mlr model

```{r}
train_df <- train_df %>%
  select(-c(PROX_CHAS))
train_sf <- train_sf %>%
  select(-c(PROX_CHAS))
test_df <- test_df %>%
  select(-c(PROX_CHAS))
test_sf <- test_sf %>%
  select(-c(PROX_CHAS))
```

```{r}
rs_mlr <- lm(RESALE_PRICE ~ 
               FLOOR_AREA_SQM +
                  STOREY_ORDER + 
               REMAINING_LEASE_MTHS +
                  PROX_CBD + 
               PROX_ELDERLYCARE + 
               PROX_HAWKER +
                  PROX_MRT + 
               PROX_PARK +
               PROX_MALL + 
                  PROX_SUPERMARKET + 
               WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + 
               WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_df)
summary(rs_mlr)
```


## Preparing coordinates data 

The code extract the x, y coordinates of the full, training and test data sets.

```{r}
coords <- st_coordinates(rs_sf)
coords_train <- st_coordinates(train_sf)
coords_test <- st_coordinates(test_sf)
```

## Decision Tree

This R code snippet trains a decision tree model using the rpart() function, employing various predictors such as floor area, storey order, remaining lease months, and proximity indicators to amenities like malls, parks, and public transportation. Additionally, binary indicators are utilized to denote the presence of amenities within specific distances from the property. The model is trained on a dataset (train_df) to predict resale prices (RESALE_PRICE). After fitting the model, the summary() function is called to provide a concise overview of the model's structure and potentially its predictive performance. Overall, the code aims to build and assess a decision tree model for predicting property resale prices based on a range of relevant features.


```{r}
rs_rp <- rpart(RESALE_PRICE ~ 
               FLOOR_AREA_SQM +
                  STOREY_ORDER + 
               REMAINING_LEASE_MTHS +
                  PROX_CBD + 
               PROX_ELDERLYCARE + 
               PROX_HAWKER +
                  PROX_MRT + 
               PROX_PARK +
               PROX_MALL + 
                  PROX_SUPERMARKET + 
               WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + 
               WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_df)
summary(rs_rp)
```

## Plotting the decision tree

```{r}
rpart.plot(rs_rp)
```


The ranger() function in R is used for building Random Forest models, which are a type of ensemble learning method.

A Random Forest model is a popular ensemble learning technique used for both classification and regression tasks in machine learning. It's an ensemble method because it combines the predictions from multiple individual models to produce a more robust and accurate final prediction.

Here's how a Random Forest model works:

Building Multiple Decision Trees: Instead of relying on a single decision tree, a Random Forest constructs a multitude of decision trees during training. Each tree is trained independently on a random subset of the training data and a random subset of the features.

Random Feature Selection: At each node of each decision tree, a random subset of features is considered for splitting. This randomness helps to decorrelate the trees, making them more diverse and less prone to overfitting.

Voting or Averaging: For regression tasks, the predictions of all trees are averaged to produce the final prediction. For classification tasks, each tree "votes" for a class, and the most popular class is chosen as the final prediction.

Bootstrap Aggregating (Bagging): Random Forest uses a technique called bagging, which involves training each tree on a bootstrapped sample of the training data. This means that each tree is trained on a randomly selected subset of the training data with replacement.

The key advantages of Random Forest models include their robustness to overfitting, ability to handle high-dimensional data, and capability to capture complex relationships between features and the target variable. They are also less sensitive to outliers and noise in the data compared to individual decision trees. Additionally, Random Forest models provide estimates of feature importance, which can be valuable for understanding the underlying patterns in the data.

```{r}
set.seed(1234)
rs_rf <- ranger(RESALE_PRICE ~ 
               FLOOR_AREA_SQM +
                  STOREY_ORDER + 
               REMAINING_LEASE_MTHS +
                  PROX_CBD + 
               PROX_ELDERLYCARE + 
               PROX_HAWKER +
                  PROX_MRT + 
               PROX_PARK +
               PROX_MALL + 
                  PROX_SUPERMARKET + 
               WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + 
               WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_df,
               importance = "impurity")
rs_rf
```

```{r}
rs_rf <- read_rds("data/models/rs_rf.rds")
rs_rf
```

```{r}
vi <- as.data.frame(rs_rf$variable.importance)

vi$variables <- rownames(vi)
vi <- vi %>%
  rename(vi = "rs_rf$variable.importance")
```

* Important: include (stat = "identity") to aggregate the data here.
Only then it will be able to plot if not will generate error

```{r}
ggplot(data = vi,
       aes(x = vi,
           y = reorder(variables, vi))) + 
  geom_bar(stat= "identity")
```






